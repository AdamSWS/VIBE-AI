{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2159e757-6c63-4820-b867-80da7c028e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecdf7fc4-26bc-46ca-9d87-3e32897f6c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connecting to DB\n",
    "URI = f\"Your URI\"\n",
    "client = MongoClient(URI)\n",
    "db = client['youtube_comments']\n",
    "collection = db['tech_comments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa583795-03e5-4b38-b105-808b4697c811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11053\n"
     ]
    }
   ],
   "source": [
    "cursor = collection.find({})  # '_id': 0 excludes the _id field\n",
    "# cursor.collection.find({\"comment\": {\"$regex\": \"suggest\", \"$options\": \"i\"}})\n",
    "# Convert the MongoDB cursor to a list of documents\n",
    "data = list(cursor)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3b74f33-aa36-45e3-99d6-e83f9fd0cd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments = []\n",
    "\n",
    "# Iterate over all documents in the collection\n",
    "for document in collection.find():\n",
    "    if 'comments' in document:\n",
    "        for comment in document['comments']:\n",
    "            # print(comment)\n",
    "            # break\n",
    "            all_comments.append(comment.get('original_text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ebb311d3-5de7-457d-9fb3-d3663da1d3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cadmin/anaconda3/envs/Data_Science_V2/lib/python3.11/site-packages/transformers/training_args.py:1583: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 03:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=75, training_loss=3.4832389322916666, metrics={'train_runtime': 235.2633, 'train_samples_per_second': 1.275, 'train_steps_per_second': 0.319, 'total_flos': 78387609600000.0, 'train_loss': 3.4832389322916666, 'epoch': 3.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Load pre-trained GPT-2\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Add a new pad token to the tokenizer\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Update the model with the new pad token\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Prepare the dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=512):\n",
    "        self.input_ids = []\n",
    "        self.attention_masks = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for text in texts:\n",
    "            \n",
    "            # Tokenize and add padding and truncation\n",
    "            encoding = tokenizer(text, truncation=True, padding='max_length', max_length=max_length, return_tensors='pt')\n",
    "            input_ids = encoding['input_ids'].squeeze(0)  # Remove batch dimension\n",
    "            attention_mask = encoding['attention_mask'].squeeze(0)  # Remove batch dimension\n",
    "            \n",
    "            # Labels for GPT-2 are the same as the input_ids, shifted by one token\n",
    "            labels = input_ids.clone()\n",
    "            labels[labels == tokenizer.pad_token_id] = -100  # Ignore pad tokens when computing loss\n",
    "            \n",
    "            self.input_ids.append(input_ids)\n",
    "            self.attention_masks.append(attention_mask)\n",
    "            self.labels.append(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_masks[idx],\n",
    "            'labels': self.labels[idx]  # Provide the labels for GPT-2\n",
    "        }\n",
    "\n",
    "# Example train data (replace with your actual text data)\n",
    "train_texts = all_comments[0:100]  # This is just an example, use your actual data\n",
    "\n",
    "# Initialize dataset\n",
    "train_dataset = TextDataset(train_texts, tokenizer)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # number of training epochs\n",
    "    per_device_train_batch_size=4,   # batch size per device during training\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    no_cuda=True,                    # Set to True since we're using CPU (disable CUDA)\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the pre-trained model\n",
    "    args=training_args,                  # training arguments\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d892c077-2080-43b6-864e-7c7ea0b4c715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cadmin/anaconda3/envs/Data_Science_V2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/cadmin/anaconda3/envs/Data_Science_V2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggest a tech idea to help you understand the basics of Python.\n",
      "I'm a Python beginner and I'm starting to learn Python from scratch. I am a beginner with a passion for coding and a great way to get started. Thanks for sharing your\n"
     ]
    }
   ],
   "source": [
    "#Sample prompt\n",
    "prompt_text = \"Suggest a tech idea\"\n",
    "\n",
    "# Tokenize the prompt text\n",
    "input_ids = tokenizer.encode(prompt_text, return_tensors='pt')\n",
    "\n",
    "# Generate text from the model\n",
    "output = model.generate(input_ids, \n",
    "                        max_length=50,    # Maximum length of the generated sequence\n",
    "                        num_return_sequences=1,  # Number of generated sequences\n",
    "                        no_repeat_ngram_size=2,  # Avoid repeating n-grams\n",
    "                        temperature=0.7,  # Sampling temperature (lower means more deterministic)\n",
    "                        top_k=50,         # Top-k sampling\n",
    "                        top_p=0.95,       # Top-p (nucleus) sampling\n",
    "                        pad_token_id=tokenizer.pad_token_id)  # Padding token ID\n",
    "\n",
    "# Decode the generated output and print it\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bcb7cc-d3ac-4e47-990d-ee61e8e530d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
